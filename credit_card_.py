# -*- coding: utf-8 -*-
"""Credit_Card .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jxSW0JQ9K4DJhUNkh1pkcA-MsOy1AlCR
"""

# Importing Libraries 
import pandas as pd
import numpy as np

# Imported Csv file into DataFrame using Pandas
df = pd.read_csv("Credit_card.csv")
df

# Finding Missing Values
df.isnull().sum()

# observing Statistical Analysis
df.describe()

"""# dropping unwanted columns and rows"""

df = df.drop("Unnamed: 0",  axis = 1)

df= df.iloc[1:,:]
df

# Converting the object datatype into integer and Category

df['X1'] = df['X1'].astype('int64')
df["X2"] = df["X2"].astype("category")
df["X3"] = df["X3"].astype("category")
df["X4"] = df["X4"].astype("category")
df["X5"] = df["X5"].astype("int64")
df['X6'] = df['X6'].astype('category')
df['X7'] = df['X7'].astype('category')
df['X8'] = df['X8'].astype('category')
df['X9'] = df['X9'].astype('category')
df['X10'] = df['X10'].astype('category')
df['X11'] = df['X11'].astype('category')
df['X12'] = df['X12'].astype('int64')
df['X13'] = df['X13'].astype('int64')
df['X14'] = df['X14'].astype('int64')
df['X15'] = df['X15'].astype('int64')
df['X16'] = df['X16'].astype('int64')
df['X17'] = df['X17'].astype('int64')
df['X18'] = df['X18'].astype('int64')
df['X19'] = df['X19'].astype('int64')
df['X20'] = df['X20'].astype('int64')
df['X21'] = df['X21'].astype('int64')
df['X22'] = df['X22'].astype('int64')
df['X23'] = df['X23'].astype('int64')
df['Y'] = df['Y'].astype('category')

df.info()

# Correlation_Matrix for Dataset
df_corr =df.corr()
df_corr.shape

"""# Heatmap for dataset"""

import seaborn as sns

sns.heatmap(df_corr, annot=True, annot_kws={"size": 5})

# Assigning the coulmns to X variable and drop the target coulmn
X = df.drop("Y", axis =1)
X

"""# Standardization """

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_std = scaler.fit_transform(X)

X_std

"""# Validation"""

from sklearn.model_selection import train_test_split
X = X_std
y = df["Y"]
X_train,X_test,y_train,y_test = train_test_split(X, y, test_size = 0.2, random_state = 2)

X_train.shape,y_train.shape

# importing KNN model
from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier(n_neighbors = 5)
model.fit(X_train, y_train)

# train accuracy
pred = model.predict(X_train)
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
print(classification_report(y_train, pred))

# test accuracy
predictions = model.predict(X_test)
print(classification_report(y_test, predictions))

print(confusion_matrix(y_test, predictions))

from sklearn.metrics import precision_score

Class_1 =  precision_score(y_train, pred, average='macro', labels=[1])
Class_1

Class_0 =  precision_score(y_train, pred, average='macro', labels=[0])
Class_0

p = ((Class_0)*100/2 + (Class_1)*100/2)
p

"""# Optimizing K value using elbow method"""

error_rate = []
for i in range(1,101):
 knn = KNeighborsClassifier(n_neighbors=i)
 knn.fit(X_train,y_train)
 pred_i = knn.predict(X_test)
 error_rate.append(np.mean(pred_i != y_test))

plt.figure(figsize=(10,6))
plt.plot(range(1,101),error_rate,color='blue', linestyle='dashed', 
         marker='o',markerfacecolor='red', markersize=10)
plt.title('Error Rate vs. K Value')
plt.xlabel('K')
plt.ylabel('Error Rate')
print("Minimum error:-",min(error_rate),"at K =",error_rate.index(min(error_rate)))

acc = []
# Will take some time
from sklearn import metrics
for i in range(1,101):
    neigh = KNeighborsClassifier(n_neighbors = i).fit(X_train,y_train)
    yhat = neigh.predict(X_test)
    acc.append(metrics.accuracy_score(y_test, yhat))
    
plt.figure(figsize=(10,6))
plt.plot(range(1,101),acc,color = 'blue',linestyle='dashed', 
         marker='o',markerfacecolor='red', markersize=10)
plt.title('accuracy vs. K Value')
plt.xlabel('K')
plt.ylabel('Accuracy')
print("Maximum accuracy:-",max(acc),"at K =",acc.index(max(acc)))

"""# using Feature Selection method"""

from numpy import mean
from numpy import std
from sklearn.datasets import make_classification
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.feature_selection import RFE
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import Pipeline
# create pipeline
rfe = RFE(estimator=KNeighborsClassifier(), n_features_to_select=5)
model = KNeighborsClassifier()
pipeline = Pipeline(steps=[('s',rfe),('m',model)])
# evaluate model
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
n_scores = cross_val_score(KNeighborsClassifier(n_neighbors=10), X_train, y_train,cv=5,scoring='accuracy')
print(n_scores)

from sklearn.model_selection import GridSearchCV
#create new a knn model
knn2 = KNeighborsClassifier()
#create a dictionary of all values we want to test for n_neighbors
param_grid = {'n_neighbors' : np.arange(1, 25)}
#use gridsearch to test all values for n_neighbors
knn_gscv = GridSearchCV(knn2, param_grid, cv=5)
#fit model to data
knn_gscv.fit(X, y)

knn_gscv.best_params_

knn_gscv.best_score_

from sklearn.metrics import make_scorer
!pip install mlxtend 
from mlxtend.feature_selection import SequentialFeatureSelector as sfs
def P(y_true, y_pred):
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    n = sum((tn, fp, fn, tp))
    return (((tn/n) * 100) // 2) + (((tp/n) * 100) // 2)
P_scoring = make_scorer(P, greater_is_better=True)
foward_feature_selection=sfs(KNeighborsClassifier(n_jobs=-1),k_features=(1,23),forward=True,floating=False, verbose=2,scoring=P_scoring).fit(X_train,y_train)

foward_feature_selection.k_feature_names_

foward_feature_selection.k_score_

# X1,X5,X12 has highest P value.

